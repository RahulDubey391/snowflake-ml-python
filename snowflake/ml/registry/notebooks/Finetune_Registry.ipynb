{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fa0e355f",
   "metadata": {},
   "source": [
    "1. Create a conda python3.8 conda env\n",
    "`conda create --name snowml python=3.8`\n",
    "\n",
    "2. You need to install these packages locally\n",
    " * peft \n",
    " * transformers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "255a02dd-9208-4489-9468-fb98231e859b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Ignoring invalid distribution -ackaging (/opt/conda/envs/pytorch/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0mCollecting snowflake-snowpark-python==1.8.0\n",
      "  Using cached snowflake_snowpark_python-1.8.0-py3-none-any.whl (326 kB)\n",
      "Collecting setuptools>=40.6.0 (from snowflake-snowpark-python==1.8.0)\n",
      "  Using cached setuptools-68.2.2-py3-none-any.whl (807 kB)\n",
      "Collecting wheel (from snowflake-snowpark-python==1.8.0)\n",
      "  Using cached wheel-0.41.3-py3-none-any.whl (65 kB)\n",
      "Collecting snowflake-connector-python<4.0.0,>=3.2.0 (from snowflake-snowpark-python==1.8.0)\n",
      "  Using cached snowflake_connector_python-3.3.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (26.6 MB)\n",
      "Collecting pyyaml (from snowflake-snowpark-python==1.8.0)\n",
      "  Using cached PyYAML-6.0.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (705 kB)\n",
      "Collecting cloudpickle<=2.0.0,>=1.6.0 (from snowflake-snowpark-python==1.8.0)\n",
      "  Using cached cloudpickle-2.0.0-py3-none-any.whl (25 kB)\n",
      "Collecting asn1crypto<2.0.0,>0.24.0 (from snowflake-connector-python<4.0.0,>=3.2.0->snowflake-snowpark-python==1.8.0)\n",
      "  Using cached asn1crypto-1.5.1-py2.py3-none-any.whl (105 kB)\n",
      "Collecting cffi<2.0.0,>=1.9 (from snowflake-connector-python<4.0.0,>=3.2.0->snowflake-snowpark-python==1.8.0)\n",
      "  Using cached cffi-1.16.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (443 kB)\n",
      "Collecting cryptography<42.0.0,>=3.1.0 (from snowflake-connector-python<4.0.0,>=3.2.0->snowflake-snowpark-python==1.8.0)\n",
      "  Using cached cryptography-41.0.5-cp37-abi3-manylinux_2_28_x86_64.whl (4.4 MB)\n",
      "Collecting oscrypto<2.0.0 (from snowflake-connector-python<4.0.0,>=3.2.0->snowflake-snowpark-python==1.8.0)\n",
      "  Using cached oscrypto-1.3.0-py2.py3-none-any.whl (194 kB)\n",
      "Collecting pyOpenSSL<24.0.0,>=16.2.0 (from snowflake-connector-python<4.0.0,>=3.2.0->snowflake-snowpark-python==1.8.0)\n",
      "  Using cached pyOpenSSL-23.3.0-py3-none-any.whl (58 kB)\n",
      "Collecting pycryptodomex!=3.5.0,<4.0.0,>=3.2 (from snowflake-connector-python<4.0.0,>=3.2.0->snowflake-snowpark-python==1.8.0)\n",
      "  Using cached pycryptodomex-3.19.0-cp35-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.1 MB)\n",
      "Collecting pyjwt<3.0.0 (from snowflake-connector-python<4.0.0,>=3.2.0->snowflake-snowpark-python==1.8.0)\n",
      "  Using cached PyJWT-2.8.0-py3-none-any.whl (22 kB)\n",
      "Collecting pytz (from snowflake-connector-python<4.0.0,>=3.2.0->snowflake-snowpark-python==1.8.0)\n",
      "  Using cached pytz-2023.3.post1-py2.py3-none-any.whl (502 kB)\n",
      "Collecting requests<3.0.0 (from snowflake-connector-python<4.0.0,>=3.2.0->snowflake-snowpark-python==1.8.0)\n",
      "  Using cached requests-2.31.0-py3-none-any.whl (62 kB)\n",
      "Collecting packaging (from snowflake-connector-python<4.0.0,>=3.2.0->snowflake-snowpark-python==1.8.0)\n",
      "  Using cached packaging-23.2-py3-none-any.whl (53 kB)\n",
      "Collecting charset-normalizer<4,>=2 (from snowflake-connector-python<4.0.0,>=3.2.0->snowflake-snowpark-python==1.8.0)\n",
      "  Using cached charset_normalizer-3.3.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (142 kB)\n",
      "Collecting idna<4,>=2.5 (from snowflake-connector-python<4.0.0,>=3.2.0->snowflake-snowpark-python==1.8.0)\n",
      "  Using cached idna-3.4-py3-none-any.whl (61 kB)\n",
      "Collecting urllib3<1.27,>=1.21.1 (from snowflake-connector-python<4.0.0,>=3.2.0->snowflake-snowpark-python==1.8.0)\n",
      "  Using cached urllib3-1.26.18-py2.py3-none-any.whl (143 kB)\n",
      "Collecting certifi>=2017.4.17 (from snowflake-connector-python<4.0.0,>=3.2.0->snowflake-snowpark-python==1.8.0)\n",
      "  Using cached certifi-2023.7.22-py3-none-any.whl (158 kB)\n",
      "Collecting typing-extensions<5,>=4.3 (from snowflake-connector-python<4.0.0,>=3.2.0->snowflake-snowpark-python==1.8.0)\n",
      "  Using cached typing_extensions-4.8.0-py3-none-any.whl (31 kB)\n",
      "Collecting filelock<4,>=3.5 (from snowflake-connector-python<4.0.0,>=3.2.0->snowflake-snowpark-python==1.8.0)\n",
      "  Using cached filelock-3.13.1-py3-none-any.whl (11 kB)\n",
      "Collecting sortedcontainers>=2.4.0 (from snowflake-connector-python<4.0.0,>=3.2.0->snowflake-snowpark-python==1.8.0)\n",
      "  Using cached sortedcontainers-2.4.0-py2.py3-none-any.whl (29 kB)\n",
      "Collecting platformdirs<4.0.0,>=2.6.0 (from snowflake-connector-python<4.0.0,>=3.2.0->snowflake-snowpark-python==1.8.0)\n",
      "  Using cached platformdirs-3.11.0-py3-none-any.whl (17 kB)\n",
      "Collecting tomlkit (from snowflake-connector-python<4.0.0,>=3.2.0->snowflake-snowpark-python==1.8.0)\n",
      "  Using cached tomlkit-0.12.1-py3-none-any.whl (37 kB)\n",
      "Collecting pycparser (from cffi<2.0.0,>=1.9->snowflake-connector-python<4.0.0,>=3.2.0->snowflake-snowpark-python==1.8.0)\n",
      "  Using cached pycparser-2.21-py2.py3-none-any.whl (118 kB)\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -ackaging (/opt/conda/envs/pytorch/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0mInstalling collected packages: sortedcontainers, pytz, asn1crypto, wheel, urllib3, typing-extensions, tomlkit, setuptools, pyyaml, pyjwt, pycryptodomex, pycparser, platformdirs, packaging, oscrypto, idna, filelock, cloudpickle, charset-normalizer, certifi, requests, cffi, cryptography, pyOpenSSL, snowflake-connector-python, snowflake-snowpark-python\n",
      "  Attempting uninstall: sortedcontainers\n",
      "    Found existing installation: sortedcontainers 2.4.0\n",
      "    Uninstalling sortedcontainers-2.4.0:\n",
      "      Successfully uninstalled sortedcontainers-2.4.0\n",
      "  Attempting uninstall: pytz\n",
      "    Found existing installation: pytz 2023.3.post1\n",
      "    Uninstalling pytz-2023.3.post1:\n",
      "      Successfully uninstalled pytz-2023.3.post1\n",
      "  Attempting uninstall: asn1crypto\n",
      "    Found existing installation: asn1crypto 1.5.1\n",
      "    Uninstalling asn1crypto-1.5.1:\n",
      "      Successfully uninstalled asn1crypto-1.5.1\n",
      "  Attempting uninstall: wheel\n",
      "    Found existing installation: wheel 0.41.3\n",
      "    Uninstalling wheel-0.41.3:\n",
      "      Successfully uninstalled wheel-0.41.3\n",
      "  Attempting uninstall: urllib3\n",
      "    Found existing installation: urllib3 1.26.18\n",
      "    Uninstalling urllib3-1.26.18:\n",
      "      Successfully uninstalled urllib3-1.26.18\n",
      "  Attempting uninstall: typing-extensions\n",
      "    Found existing installation: typing_extensions 4.8.0\n",
      "    Uninstalling typing_extensions-4.8.0:\n",
      "      Successfully uninstalled typing_extensions-4.8.0\n",
      "  Attempting uninstall: tomlkit\n",
      "    Found existing installation: tomlkit 0.12.1\n",
      "    Uninstalling tomlkit-0.12.1:\n",
      "      Successfully uninstalled tomlkit-0.12.1\n",
      "  Attempting uninstall: setuptools\n",
      "    Found existing installation: setuptools 68.2.2\n",
      "    Uninstalling setuptools-68.2.2:\n",
      "      Successfully uninstalled setuptools-68.2.2\n",
      "  Attempting uninstall: pyyaml\n",
      "    Found existing installation: PyYAML 6.0.1\n",
      "    Uninstalling PyYAML-6.0.1:\n",
      "      Successfully uninstalled PyYAML-6.0.1\n",
      "  Attempting uninstall: pyjwt\n",
      "    Found existing installation: PyJWT 2.8.0\n",
      "    Uninstalling PyJWT-2.8.0:\n",
      "      Successfully uninstalled PyJWT-2.8.0\n",
      "  Attempting uninstall: pycryptodomex\n",
      "    Found existing installation: pycryptodomex 3.19.0\n",
      "    Uninstalling pycryptodomex-3.19.0:\n",
      "      Successfully uninstalled pycryptodomex-3.19.0\n",
      "  Attempting uninstall: pycparser\n",
      "    Found existing installation: pycparser 2.21\n",
      "    Uninstalling pycparser-2.21:\n",
      "      Successfully uninstalled pycparser-2.21\n",
      "  Attempting uninstall: platformdirs\n",
      "    Found existing installation: platformdirs 3.11.0\n",
      "    Uninstalling platformdirs-3.11.0:\n",
      "      Successfully uninstalled platformdirs-3.11.0\n",
      "  Attempting uninstall: packaging\n",
      "    Found existing installation: packaging 23.2\n",
      "    Uninstalling packaging-23.2:\n",
      "      Successfully uninstalled packaging-23.2\n",
      "  Attempting uninstall: oscrypto\n",
      "    Found existing installation: oscrypto 1.3.0\n",
      "    Uninstalling oscrypto-1.3.0:\n",
      "      Successfully uninstalled oscrypto-1.3.0\n",
      "  Attempting uninstall: idna\n",
      "    Found existing installation: idna 3.4\n",
      "    Uninstalling idna-3.4:\n",
      "      Successfully uninstalled idna-3.4\n",
      "  Attempting uninstall: filelock\n",
      "    Found existing installation: filelock 3.12.2\n",
      "    Uninstalling filelock-3.12.2:\n",
      "      Successfully uninstalled filelock-3.12.2\n",
      "  Attempting uninstall: cloudpickle\n",
      "    Found existing installation: cloudpickle 2.0.0\n",
      "    Uninstalling cloudpickle-2.0.0:\n",
      "      Successfully uninstalled cloudpickle-2.0.0\n",
      "  Attempting uninstall: charset-normalizer\n",
      "    Found existing installation: charset-normalizer 3.1.0\n",
      "    Uninstalling charset-normalizer-3.1.0:\n",
      "      Successfully uninstalled charset-normalizer-3.1.0\n",
      "  Attempting uninstall: certifi\n",
      "    Found existing installation: certifi 2023.5.7\n",
      "    Uninstalling certifi-2023.5.7:\n",
      "      Successfully uninstalled certifi-2023.5.7\n",
      "  Attempting uninstall: requests\n",
      "    Found existing installation: requests 2.31.0\n",
      "    Uninstalling requests-2.31.0:\n",
      "      Successfully uninstalled requests-2.31.0\n",
      "  Attempting uninstall: cffi\n",
      "    Found existing installation: cffi 1.15.1\n",
      "    Uninstalling cffi-1.15.1:\n",
      "      Successfully uninstalled cffi-1.15.1\n",
      "  Attempting uninstall: cryptography\n",
      "    Found existing installation: cryptography 39.0.2\n",
      "    Uninstalling cryptography-39.0.2:\n",
      "      Successfully uninstalled cryptography-39.0.2\n",
      "  Attempting uninstall: pyOpenSSL\n",
      "    Found existing installation: pyOpenSSL 23.2.0\n",
      "    Uninstalling pyOpenSSL-23.2.0:\n",
      "      Successfully uninstalled pyOpenSSL-23.2.0\n",
      "  Attempting uninstall: snowflake-connector-python\n",
      "    Found existing installation: snowflake-connector-python 3.3.1\n",
      "    Uninstalling snowflake-connector-python-3.3.1:\n",
      "      Successfully uninstalled snowflake-connector-python-3.3.1\n",
      "  Attempting uninstall: snowflake-snowpark-python\n",
      "    Found existing installation: snowflake-snowpark-python 1.9.0\n",
      "    Uninstalling snowflake-snowpark-python-1.9.0:\n",
      "      Successfully uninstalled snowflake-snowpark-python-1.9.0\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "triton 2.0.0 requires cmake, which is not installed.\n",
      "triton 2.0.0 requires lit, which is not installed.\n",
      "awscli 1.27.151 requires botocore==1.29.151, but you have botocore 1.31.17 which is incompatible.\n",
      "awscli 1.27.151 requires PyYAML<5.5,>=3.10, but you have pyyaml 6.0.1 which is incompatible.\n",
      "sagemaker 2.164.0 requires cloudpickle==2.2.1, but you have cloudpickle 2.0.0 which is incompatible.\n",
      "sagemaker 2.164.0 requires PyYAML==6.0, but you have pyyaml 6.0.1 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed asn1crypto-1.5.1 certifi-2023.7.22 cffi-1.16.0 charset-normalizer-3.3.2 cloudpickle-2.0.0 cryptography-41.0.5 filelock-3.13.1 idna-3.4 oscrypto-1.3.0 packaging-23.2 platformdirs-3.11.0 pyOpenSSL-23.3.0 pycparser-2.21 pycryptodomex-3.19.0 pyjwt-2.8.0 pytz-2023.3.post1 pyyaml-6.0.1 requests-2.31.0 setuptools-68.2.2 snowflake-connector-python-3.3.1 snowflake-snowpark-python-1.8.0 sortedcontainers-2.4.0 tomlkit-0.12.1 typing-extensions-4.8.0 urllib3-1.26.18 wheel-0.41.3\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade --force-reinstall snowflake-snowpark-python==1.8.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "1ed66db9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Ignoring invalid distribution -ackaging (/opt/conda/envs/pytorch/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0mProcessing /home/ubuntu/snowml/bazel-bin/snowflake/ml/snowflake_ml_python-1.0.12-py3-none-any.whl\n",
      "Installing collected packages: snowflake-ml-python\n",
      "  Attempting uninstall: snowflake-ml-python\n",
      "    Found existing installation: snowflake-ml-python 1.0.12\n",
      "    Uninstalling snowflake-ml-python-1.0.12:\n",
      "      Successfully uninstalled snowflake-ml-python-1.0.12\n",
      "Successfully installed snowflake-ml-python-1.0.12\n"
     ]
    }
   ],
   "source": [
    "!pip install --force-reinstall --no-deps /home/ubuntu/snowml/bazel-bin/snowflake/ml/snowflake_ml_python-1.0.12-py3-none-any.whl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "292e9f48",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4c6b1310-9941-4ba3-b126-6e58c01fb613",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Ignoring invalid distribution -ackaging (/opt/conda/envs/pytorch/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0msnowflake-snowpark-python     1.8.0\n"
     ]
    }
   ],
   "source": [
    "! pip list | grep snowpark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7585077b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from snowflake.snowpark import Session\n",
    "from snowflake.ml.utils.connection_params import SnowflakeLoginOptions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a0294ba",
   "metadata": {},
   "source": [
    "Connection config available at ~/.snowsql/config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f876232e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "SnowflakeLoginOptions() is in private preview since 0.2.0. Do not use it in production. \n"
     ]
    }
   ],
   "source": [
    "session = Session.builder.configs(SnowflakeLoginOptions('connections.demo')).create()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c6aee8c9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('\"HALU_FT\"', '\"PUBLIC\"')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "session.get_current_database(), session.get_current_schema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "72c16c14",
   "metadata": {},
   "outputs": [],
   "source": [
    "REGISTRY_DATABASE_NAME = \"HALU_MR\"\n",
    "REGISTRY_SCHEMA_NAME = \"PUBLIC\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c420807b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:snowflake.snowpark:create_model_registry() is in private preview since 0.2.0. Do not use it in production. \n",
      "WARNING:absl:The database HALU_MR already exists. Skipping creation.\n",
      "WARNING:absl:The schema HALU_MR.PUBLIC already exists. Skipping creation.\n"
     ]
    }
   ],
   "source": [
    "from snowflake.ml.registry import model_registry\n",
    "\n",
    "model_registry.create_model_registry(\n",
    "    session=session, database_name=REGISTRY_DATABASE_NAME, schema_name=REGISTRY_SCHEMA_NAME\n",
    ")\n",
    "registry = model_registry.ModelRegistry(\n",
    "    session=session, database_name=REGISTRY_DATABASE_NAME, schema_name=REGISTRY_SCHEMA_NAME\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "0adc9637",
   "metadata": {},
   "outputs": [],
   "source": [
    "from snowflake.ml.model.models import llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "18323af6",
   "metadata": {},
   "outputs": [],
   "source": [
    "options = llm.LLMOptions(\n",
    "    token=\"...\",\n",
    "    max_batch_size=20,\n",
    ")\n",
    "model = llm.LLM(\n",
    "    model_id_or_path=\"/home/ubuntu/projects/test_ft_weights\",\n",
    "    options=options\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "dac3fc56",
   "metadata": {},
   "outputs": [],
   "source": [
    "svc_model = registry.log_model(\n",
    "    model_name='build_demo_1101',\n",
    "    model_version='v5',\n",
    "    model=model,\n",
    "    options={\"embed_local_ml_library\": True},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "b17b1fbb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:snowflake.ml.model._deploy_client.snowservice.deploy:Debug model is enabled, deployment artifacts will be available in /tmp/tmpqkmmoahf\n",
      "WARNING:snowflake.ml.model._deploy_client.snowservice.deploy:Building the Docker image and deploying to Snowpark Container Service. This process may take a few minutes.\n",
      "WARNING:snowflake.ml.model._deploy_client.snowservice.deploy:Image successfully built! For future model deployments, the image will be reused if possible, saving model deployment time. To enforce using the same image, include 'prebuilt_snowflake_image': 'sfengineering-servicesnow.registry.snowflakecomputing.com/halu_ft_db/public/haul_repo/50e52d564ecc126d1f53452aa4dd734efa4e3a0a:latest' in the deploy() function's options.\n",
      "WARNING:urllib3.connectionpool:Retrying (Retry(total=2, connect=None, read=None, redirect=None, status=None)) after connection broken by 'ProtocolError('Connection aborted.', RemoteDisconnected('Remote end closed connection without response'))': /login\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'name': 'HALU_MR.PUBLIC.build_demo_1101_4',\n",
       " 'platform': <TargetPlatform.SNOWPARK_CONTAINER_SERVICES: 'SNOWPARK_CONTAINER_SERVICES'>,\n",
       " 'target_method': 'infer',\n",
       " 'signature': ModelSignature(\n",
       "                     inputs=[\n",
       "                         FeatureSpec(dtype=DataType.STRING, name='input')\n",
       "                     ],\n",
       "                     outputs=[\n",
       "                         FeatureSpec(dtype=DataType.STRING, name='generated_text')\n",
       "                     ]\n",
       "                 ),\n",
       " 'options': {'compute_pool': 'BUILD_2023_POOL',\n",
       "  'num_gpus': 1,\n",
       "  'image_repo': 'sfengineering-servicesnow.registry.snowflakecomputing.com/halu_ft_db/public/haul_repo',\n",
       "  'enable_remote_image_build': True,\n",
       "  'model_in_image': True,\n",
       "  'debug_mode': True},\n",
       " 'details': {'image_name': 'sfengineering-servicesnow.registry.snowflakecomputing.com/halu_ft_db/public/haul_repo/50e52d564ecc126d1f53452aa4dd734efa4e3a0a:latest',\n",
       "  'service_spec': 'spec:\\n  container:\\n  - env:\\n      NUM_WORKERS: 1\\n      SNOWML_USE_GPU: true\\n      TARGET_METHOD: infer\\n      _CONCURRENT_REQUESTS_MAX: 1\\n    image: sfengineering-servicesnow.registry.snowflakecomputing.com/halu_ft_db/public/haul_repo/50e52d564ecc126d1f53452aa4dd734efa4e3a0a:latest\\n    name: inference-server\\n    readinessProbe:\\n      path: /health\\n      port: 5000\\n    resources:\\n      limits:\\n        nvidia.com/gpu: 1\\n      requests:\\n        nvidia.com/gpu: 1\\n    volumeMounts:\\n    - mountPath: /local/user/vol1\\n      name: vol1\\n  endpoint:\\n  - name: predict\\n    port: 5000\\n  volume:\\n  - name: vol1\\n    source: local\\n',\n",
       "  'service_function_sql': \"\\n            CREATE OR REPLACE FUNCTION HALU_MR.PUBLIC.build_demo_1101_4(input OBJECT)\\n                RETURNS OBJECT\\n                SERVICE=HALU_MR.PUBLIC.service_3b9880c078d711ee861c06f9498c0da3\\n                ENDPOINT=predict\\n                MAX_BATCH_ROWS = 20\\n                AS '/predict'\\n            \"}}"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from snowflake.ml.model import deploy_platforms\n",
    "\n",
    "deployment_options = {\n",
    "    \"compute_pool\": 'BUILD_2023_POOL',\n",
    "    \"num_gpus\": 1,\n",
    "    \"image_repo\": 'sfengineering-servicesnow.registry.snowflakecomputing.com/halu_ft_db/public/haul_repo',\n",
    "    \"enable_remote_image_build\": True,\n",
    "    \"model_in_image\": True,\n",
    "    \"debug_mode\": True,\n",
    "    #'prebuilt_snowflake_image': 'sfengineering-servicesnow.registry.snowflakecomputing.com/halu_ft_db/public/haul_repo/93d14fc687640746235f8f880a6af8c730ce3eaf:latest'\n",
    "}\n",
    "        \n",
    "deploy_info = svc_model.deploy(\n",
    "    deployment_name=\"build_demo_1101_4\",\n",
    "    platform=deploy_platforms.TargetPlatform.SNOWPARK_CONTAINER_SERVICES,\n",
    "    permanent=True,\n",
    "    options=deployment_options\n",
    ")\n",
    "deploy_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f14753df-6fdc-422e-864d-10d93fbc05a6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "9475d6cd-5222-4bcb-9883-9d8924354d6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "PROMPT_TEMPLATE = \"\"\"\n",
    "\n",
    "[INST] <<SYS>>\n",
    "Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
    "<</SYS>>\n",
    "### Instruction:\n",
    "Extract JSON response with 'location' and 'toy_list' as keys. Start response by \"{\".\n",
    "'location': Location of the caller. Include city only.\n",
    "'toy_list\": List of toy names from the caller.\n",
    "\n",
    "### Input:\n",
    "\"\"\"\n",
    "\n",
    "def build_prompt(input):\n",
    "    return PROMPT_TEMPLATE + input + \"\\n[/INST]\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "484f44fe-bf03-497d-97b3-147fcb4074c3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b25baf1c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "2a84b44b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d067a009-567d-4869-9e8a-44694e169cc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_json('/home/ubuntu/projects/v8.jsonl', lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "fdbfa6ef-e179-44e1-898b-8c103cf09d4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfl = df['transcript'].to_list()[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "c484ec44-672d-4269-830b-42ec037cef13",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompts = [build_prompt(t) for t in dfl]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "e8377d97-a70d-4709-b1c7-ea638634a557",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_df = pd.DataFrame({'input': prompts})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3477c070-f067-471e-83b1-302dfec392b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "res = svc_model.predict(\n",
    "    deployment_name='build_demo_1101_2',\n",
    "    data=input_df\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3df9106f-30f4-4de5-b731-68d3d71901e9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac42369c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e0da3cd-c983-4b12-b7ca-bc038a75ff9b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fb574d1-2671-46d4-baa4-659951b1f4cc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4afd2ce-aaec-43d1-8961-a484964e2997",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "702e9749-e8e3-432c-9fb6-d083794fbe0b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65f84287-550f-4190-94c0-59b16aa64880",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5babe0a7-1272-4ca0-8f23-d9c57da21fce",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2b93fa3-97ae-422a-949d-5f3d72037ad9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a4f591b-48d8-4187-8ffa-1ba747800ee1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0ac832c-e8c9-4083-9429-e8050dd2b215",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "db0e8d17",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_df = pd.DataFrame({'input': [sample, sample, sample]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "3a98eabd",
   "metadata": {},
   "outputs": [],
   "source": [
    "res = svc_model.predict(\n",
    "    deployment_name='halu_ft_deploy_1',\n",
    "    data=input_df\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "f32e6498",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_colwidth', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "a467afb6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>generated_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>{\"toy_list\": [\"Fisher-Price Little People Mickey and Friends\"], \"location\": \"Perth\"}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>{\"toy_list\": [\"Fisher-Price Little People Mickey and Friends\"], \"location\": \"Perth\"}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>{\"toy_list\": [\"Fisher-Price Little People Mickey and Friends\"], \"location\": \"Perth\"}</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                          generated_text\n",
       "0   {\"toy_list\": [\"Fisher-Price Little People Mickey and Friends\"], \"location\": \"Perth\"}\n",
       "1   {\"toy_list\": [\"Fisher-Price Little People Mickey and Friends\"], \"location\": \"Perth\"}\n",
       "2   {\"toy_list\": [\"Fisher-Price Little People Mickey and Friends\"], \"location\": \"Perth\"}"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
